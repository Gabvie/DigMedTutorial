{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Textanalyse mit dem nltk-Package"
      ],
      "metadata": {
        "id": "ZDckrP-IEAJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vorbereitung\n",
        "\n",
        "Zunächst verbinden wir das Notebook wieder mit dem Google-Drive, um auf Dateien zugreifen zu können"
      ],
      "metadata": {
        "id": "2PoYLN6yj63K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wyTPGk_vWTth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages\n",
        "\n",
        "Python eignet sich deshalb so gut für Textverarbeitung, weil es viele vorgefertigte Packages gibt, die man relativ einfach anwenden kann.\n",
        "\n",
        "Diese Packages muss man zunächst vorinstallieren und dann mit dem Befehl `import` importieren\n",
        "\n"
      ],
      "metadata": {
        "id": "cmnr3LnxEFTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zunächst installieren wir das `nltk`-Package, das viele Funktionen und Methoden zum Natural Language Processing bereithält, mit `pip`"
      ],
      "metadata": {
        "id": "rNipKvTGEjxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "SXQbdT0JD_v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dann importieren wir das Package:"
      ],
      "metadata": {
        "id": "Q4UblGXwEtb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "pu2D7b3uEwfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... und noch ein Import"
      ],
      "metadata": {
        "id": "6tiHi4KDd96E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader import CategorizedPlaintextCorpusReader"
      ],
      "metadata": {
        "id": "47bZIJhuWCC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Korpus anlegen\n",
        "\n",
        "Mit `nltk` kann man einzelne Text-Files verarbeiten, aber auch gleich ein Korpus mit mehreren Texten anlegen.\n",
        "\n",
        "Wir wollen die Minnesangtexte in eine Korpus importieren. Der Übersichtlichkeit halber packen wir den Pfadnamen zunächst in eine Variable"
      ],
      "metadata": {
        "id": "9NR7FFhkeDSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/DigMedTutorial/MS_nltk/'"
      ],
      "metadata": {
        "id": "8YHnndKFVQfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nun verwenden wir die Funktion `CategorizedPlaintextCorpusReader` zum Importieren, die nicht nur die Files importiert, sondern auch nach Mustern im Dateinamen zu Kategorien zusammenfasst.\n",
        "\n",
        "Sowohl was importiert wird als auch die Kategorisierung erfolgt unter Zuhilfenahme von Regular Expressions (RegEx, wird im folgenden Beispiel durch das 'r' vor dem String angezeigt). Diese dienen zur Fuzzy-Suche und können sehr umfangreich sein, hier nur das Wichtigste bzw. die hier verwendeten Platzhalter:\n",
        "\n",
        "* \\.    beliebiges Zeichen\n",
        "* \\*    das vorangehende Zeichen kommt 0 oder mehrmals vor\n",
        "* \\+    das vorangehende Zeichen kommt mindestens ein- oder mehrmals vor\n",
        "* \\?    0 oder 1 Vorkommen\n",
        "* \\d    Eine Ziffer\n",
        "* \\     Das nachfolgende Zeichen ist buchstäblich zu lesen und setzt die 'Funktion als Platzhalter außer Kraft (mit \\. kann man also einen Punkt adressieren, ohne den Schrägstrich wäre der ja beliebiges Zeichen)\n",
        "* ()    Runde Klammern gruppieren einen Teil des Ausdrucks  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5b0YNJ1tejGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ms_corpus = CategorizedPlaintextCorpusReader(path, r'.*\\.txt', cat_pattern = r'\\d_\\d\\d_(.*?)_.*')"
      ],
      "metadata": {
        "id": "_pXSBze4VjNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Einzelfiles lassen sich mit der Methode fileids() aufrufen (im Folgenden beschränke ich die Ausgabe auf die ersten 20 Texte)."
      ],
      "metadata": {
        "id": "f56G-1GLhryd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ms_corpus.fileids()[:20]"
      ],
      "metadata": {
        "id": "-GiZiGSGXAQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mit categories() werden die Kategorien ersichtlich, die schlicht die Autornamen sind."
      ],
      "metadata": {
        "id": "4kTHD4lHh74Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ms_corpus.categories()"
      ],
      "metadata": {
        "id": "rj4XPx81Wkgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Damit lassen sich nun einzelne Kategorien aufgreifen:"
      ],
      "metadata": {
        "id": "kWGPMtteiFNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ms_corpus.fileids(categories='Winli')"
      ],
      "metadata": {
        "id": "4BRoRIxadMhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mehrere Kategorien können mit einer Liste adressiert werden:"
      ],
      "metadata": {
        "id": "ZkGBiJUmiLjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ms_corpus.fileids(categories=['Winli','Steinmar'])"
      ],
      "metadata": {
        "id": "sOChCzhxcSHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mit len() kann die Länge der Liste an Texten zu einer Kategorie ausgegeben werden, also schlicht die Anzahl der Texte für eine Kategorie:"
      ],
      "metadata": {
        "id": "Jr-Fb7oFihBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(ms_corpus.fileids(categories='Winli'))"
      ],
      "metadata": {
        "id": "icsvj27aixAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Übung: Wie viele Lieder von Walther gibt es im Korpus?"
      ],
      "metadata": {
        "id": "zvLSjWGJmNWs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7_0PMe9mV6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistische Auswertung\n",
        "\n",
        "### Types und Tokens\n",
        "\n",
        "Als erstes zählen wir schlicht, wie viele Wörter im Korpus vorkommen. In der Korpuslinguistik heißen Wörter Tokens.\n",
        "\n",
        "Mit der Methode `words()` werden alle Wörter aus dem Korpus aufgegriffen, das packen wir in eine Variable, mit `len()` bestimmen wir, wie viele Wörter es sind (kann ein wenig dauern).\n",
        "\n"
      ],
      "metadata": {
        "id": "dp-kW2cAj4B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_ges = ms_corpus.words()\n",
        "print(len(tokens_ges))"
      ],
      "metadata": {
        "id": "Ggq2kkvGkzW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sehen wir mal vorsichtig in tok_ges rein:"
      ],
      "metadata": {
        "id": "aVvjSjUfmn_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_ges[:20]"
      ],
      "metadata": {
        "id": "r37-6RKdmiJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier sind noch Satzzeichen drin, die wollen wir draußen haben.\n",
        "\n",
        "Dazu können wir die Methode `isalpha()` verwenden und zwar in einer so genannten list comprehension: Wir iterieren über die Liste und nehmen nur jene Einträge, für die gilt, dass sie aus Buchstaben bestehen:\n",
        "\n"
      ],
      "metadata": {
        "id": "3I387EMXmuVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_ges_op = [w for w in tokens_ges if w.isalpha()]"
      ],
      "metadata": {
        "id": "Yg3BXeool-sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Übung:\n",
        "\n",
        "Die Anzahl der Tokens ohne Satzzeichen ist daher:"
      ],
      "metadata": {
        "id": "Uuxo6sMwnhm0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eBs7DB-nmST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types sind die unikalen Wortformen in einem Text. Der Satz \"ich bin ich\" enthält beispielsweise 3 Tokens (ich, bin, ich), aber nur 2 Types (ich, bin)"
      ],
      "metadata": {
        "id": "9t6oz7wYn-E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gleiche Listenpunkte lassen sich in Python einfach mit der Funktion `set()` gruppieren\n"
      ],
      "metadata": {
        "id": "hvczU7xLoZzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "types_ges_op = set(tokens_ges_op)\n"
      ],
      "metadata": {
        "id": "N5HwuI6YotB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aber Achtung, hier werden Wörter, die manchmal groß, manchmal klein geschrieben sind, als unterschiedlich behandelt.\n",
        "\n",
        "Daher verwandeln wir vor Anwendung von set() besser alles in Kleinbuchstaben, wieder mit einer List comprehension:\n"
      ],
      "metadata": {
        "id": "s4zTgML6o_od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "types_ges_op = set(w.lower() for w in tokens_ges_op)"
      ],
      "metadata": {
        "id": "9vhdjYK-o60n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(types_ges_op))"
      ],
      "metadata": {
        "id": "HXd3Vrgzp0-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Type-Token-Ratio\n",
        "\n",
        "Ein Standardmaß für die Beschreibung der lexikalischen Dichte ist die Type-Token-Ratio, bei der die Anzahl der Tokens zur Anzahl der Types in Beziehung gesetzt wird."
      ],
      "metadata": {
        "id": "5vG8XjSnp9et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ttr = len(types_ges_op) / len(tokens_ges_op)\n",
        "print(ttr)"
      ],
      "metadata": {
        "id": "1AV3xlhrqQDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Type-Token-Ratio misst also, wie \"komplex\" ein Text hinsichtlich seines Sprachgebrauchs ist.\n",
        "\n",
        "Das Maß hat aber einen Nachteil: Die TTR ist abhängig von der Länge eines Textes (je länger ein Text ist, desto eher wird er dieselben Wörter benutzen, die TTR sinkt also automatisch).\n",
        "\n",
        "Es gibt daher ein paar verfeinerte Methoden, mit der man das Maß normieren kann. Beim \"Measure of Textual Lexical Diversity (MTLD)\" wird die durchschnittliche Länge der Abschnitte gemessen, die über einen bestimmten Schwellwert der TTR liegen.\n",
        "\n",
        "Zur Berechnung der MTLD verwenden wir ein weiteres Package, nämlich LexicalRichness."
      ],
      "metadata": {
        "id": "tr-g8c6DrFhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexicalrichness\n",
        "from lexicalrichness import LexicalRichness\n"
      ],
      "metadata": {
        "id": "Dn3v6s7BsVrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um das Package zu verwenden, müssen wir die Wort-Liste wieder in eine einzige Textvariable zurückführen, machen wir mit join() vor (join() steht das Trennzeichen, das eingefügt wird, wenn die Listenpunkte zusammengeführt werden. ' '.join() setzt also schlicht vor jedes Wort ein Leerzeichen."
      ],
      "metadata": {
        "id": "HQ0k7iHtsmGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_ges_op_collapsed = ' '.join(tokens_ges_op)"
      ],
      "metadata": {
        "id": "QnzTuQVitYGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Darauf können wir jetzt die Funktion LexicalRichness auführen, die alles berechnet:"
      ],
      "metadata": {
        "id": "GDdlX5MXtlsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lex = LexicalRichness(tokens_ges_op_collapsed)"
      ],
      "metadata": {
        "id": "j3pddA7VtwIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens: \",lex.words, \"Types: \",lex.terms, \"TTR: \",lex.ttr)"
      ],
      "metadata": {
        "id": "sbHodKjauCVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MTLD: \", lex.mtld(threshold=0.72))"
      ],
      "metadata": {
        "id": "MWW76C17uXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xEK6WygwLzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistiken für Einzeltexte:"
      ],
      "metadata": {
        "id": "WK_ET-2Ku-hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fileid in ms_corpus.fileids():\n",
        "  tokens_op = [w for w in ms_corpus.words(fileid) if w.isalpha]\n",
        "  num_words = len(tokens_op)\n",
        "  num_types = len(set(w.lower() for w in tokens_op))\n",
        "\n",
        "  print(\"Text \",fileid, \" hat \", num_words, \" Tokens und \", num_types, \"Types\")"
      ],
      "metadata": {
        "id": "p1k1Od_dvCS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Übung: Wie sieht das Ganze für die Kategorien aus?"
      ],
      "metadata": {
        "id": "GFOdYZq3iSpc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyhZ7e3MwoWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataframe\n",
        "\n",
        "Wir machen einen Dataframe (eine Art Tabelle) mit Kennzahlen für alle Kategorien:"
      ],
      "metadata": {
        "id": "IDD4IGH-w72A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir brauchen dafür zwei neue Packages, die weit verbreiteten numpy und pandas-Packages zur Datenstrukturierung."
      ],
      "metadata": {
        "id": "wA4IOZV4xTfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ncf2LXf32vVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Wir beginnen mit einer leeren Liste:\n",
        "\n",
        "a=list()\n",
        "\n",
        "\n",
        "## ... füllen eine Zeile mit den Informationen\n",
        "\n",
        "for cats in ms_corpus.categories():\n",
        "\n",
        "\n",
        "  ### Anzahl der Zeichen\n",
        "\n",
        "  text_char = [w.lower() for w in ms_corpus.raw(categories=cats) if w.isalpha()]\n",
        "  num_chars = len(text_char)\n",
        "\n",
        "\n",
        "  ### Tokenanzahl\n",
        "  words_op = [w.lower() for w in ms_corpus.words(categories=cats) if w.isalpha()]\n",
        "  num_words = len(words_op)\n",
        "\n",
        "  ### Durchschnittliche Wortlänge\n",
        "  av_wordlen = round(num_chars/num_words,2)\n",
        "\n",
        "  ### Anzahl Types\n",
        "\n",
        "  num_types = len(set(w.lower() for w in words_op))\n",
        "\n",
        "  ### MTLD\n",
        "\n",
        "  words_op_collapsed = ' '.join(words_op)\n",
        "  lex = LexicalRichness(words_op_collapsed)\n",
        "  mtld_ind = lex.mtld(threshold=0.72)\n",
        "\n",
        "\n",
        "  ### ... und fügen die Zeile unserer Tabelle hinzu\n",
        "\n",
        "  a.append(np.array([cats, int(num_chars),int(num_words),int(num_types),int(mtld_ind), int(av_wordlen) ]))\n",
        "\n",
        "a = np.asarray(a)"
      ],
      "metadata": {
        "id": "8T_JGcHkxMuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir konvertieren die Tabelle in einen Dataframe und vergeben Spaltennamen:"
      ],
      "metadata": {
        "id": "NjGIl9tl4HG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(a,columns=['Autor','Zeichen','Tokens','Types','Mtld','Wortlänge'])"
      ],
      "metadata": {
        "id": "tNw2QFeV3H6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Zeichen'] = df['Zeichen'].astype(int)\n",
        "df['Tokens'] = df['Tokens'].astype(int)\n",
        "df['Types'] = df['Types'].astype(int)\n",
        "df['Mtld'] = df['Mtld'].astype(int)\n",
        "df['Wortlänge'] = df['Wortlänge'].astype(float)"
      ],
      "metadata": {
        "id": "U2ZjVRUz41cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "5DiUN9AV3enn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Dataframe lässt sich leicht sortieren"
      ],
      "metadata": {
        "id": "_eg2LIjW4WnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values('Tokens',ascending=False)"
      ],
      "metadata": {
        "id": "RCbxwcsT4VGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Übung: Welche Autoren haben die höchste lexikalische Diversität?"
      ],
      "metadata": {
        "id": "ecK9J8Km5tIX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5aQYVdv-5_l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisierung\n",
        "\n",
        "Wir brauchen zwei weitere Packages\n",
        "\n"
      ],
      "metadata": {
        "id": "02gVVaVF6ARl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "FVd9BXnI6m__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir sortieren den Dataframe wie vorher und nehmen nur die 20 Bestplatzierten"
      ],
      "metadata": {
        "id": "Nu3CKkq97k7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sort = df.sort_values('Tokens',ascending=False)[:20]"
      ],
      "metadata": {
        "id": "r3VYHp9I6-e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "bp = sns.barplot(x=\"Tokens\", y=\"Autor\", data=df_sort, edgecolor=\".3\",\n",
        "            linewidth=0.5,)\n",
        "bp.set(xlabel='Mtld', ylabel='Texte', title='Verteilung Textlängen')\n",
        "plt.savefig('TextenachTokens_bar.png')"
      ],
      "metadata": {
        "id": "ts42UZdh6y2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wortfrequenzen"
      ],
      "metadata": {
        "id": "M86mroOB8eYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sind leicht zu berechnen, mit der Funktion `FreqDist` wird ein frequence table erstellt, der die Zählung für jedes Wort enthält\n"
      ],
      "metadata": {
        "id": "ANxsYHx_-JvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_ges = nltk.FreqDist(w.lower() for w in tokens_ges)"
      ],
      "metadata": {
        "id": "PhQ4tFyi8jWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aus dem Frequence Table können leicht die Werte für einzelne Wörter ausgelesen werden:"
      ],
      "metadata": {
        "id": "BJxNWlEe_Gwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(fdist_ges['leit'])"
      ],
      "metadata": {
        "id": "eujfuY65_SlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oft wird es sinnvoll sein, nicht die absolute Frequenz, sondern die relative Frequenz zu bestimmen, also die Anzahl der Belege geteilt durch die Gesamtlänge des Textes:"
      ],
      "metadata": {
        "id": "--fsBCOx_cHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(fdist_ges.freq('leit'))"
      ],
      "metadata": {
        "id": "Iw8KAvnD_paM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Frequenz für mehrere Wörter bestimmen..."
      ],
      "metadata": {
        "id": "ppbX8I0Z_6G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wortliste = ['tugende','êre','mâze','zuht','muot','triuwe','schame','vuoge']\n",
        "freqliste = []\n",
        "\n",
        "\n",
        "for w in wortliste:\n",
        "    print(w, ': ', fdist_ges[w])\n",
        "    freqliste.append(fdist_ges[w])"
      ],
      "metadata": {
        "id": "k0L1I6e3__4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... und visualisieren:"
      ],
      "metadata": {
        "id": "mVhpY_RfAZ5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(wortliste, freqliste)\n",
        "plt.xticks(wortliste, rotation=45)\n",
        "plt.title('Belege Gesamtkorpus')\n",
        "plt.xlabel('Wörter')\n",
        "plt.ylabel('Belege')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HpXP7q2uAcGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Was sind die (zwanzig) häufigsten Wörter im Gesamtkorpus?"
      ],
      "metadata": {
        "id": "udYwy5jZA2NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_ges.most_common(20)"
      ],
      "metadata": {
        "id": "KQJLby2MA6JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Satzzeichen sollten raus!"
      ],
      "metadata": {
        "id": "-a0q0DU2BIJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_ges_op = nltk.FreqDist(w.lower() for w in tokens_ges_op)"
      ],
      "metadata": {
        "id": "cKLGClxmBSaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_ges_op.most_common(20)"
      ],
      "metadata": {
        "id": "Siko9wonBXGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die häufigsten Wörter in Texten sind Funktionswörter! Und zwar überproportional häufig -> Zipfs Gesetz"
      ],
      "metadata": {
        "id": "B9jwY_MwB20u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wortliste = [w[0] for w in fdist_ges_op.most_common(100)]\n",
        "freqliste = [w[1] for w in fdist_ges_op.most_common(100)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EeB7qjSUCHnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(wortliste, freqliste)\n",
        "plt.xticks(wortliste, rotation=90)\n",
        "plt.title('MFW Gesamtkorpus')\n",
        "plt.xlabel('Wörter')\n",
        "plt.ylabel('Belege')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PwoIJDY4CUJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funktionswörter ausschließen mit Stoppwortliste\n",
        "\n",
        "Zunächst muss die Stoppwortliste importiert werden:"
      ],
      "metadata": {
        "id": "M0mt32-jNW00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/My Drive/DigMedTutorial/Stoplist_GMH.txt'\n",
        "f=open(filepath,'r', encoding='utf8')\n",
        "stopwords=f.read().splitlines()\n",
        "\n"
      ],
      "metadata": {
        "id": "cx9mTJ2oOiAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sieht so aus:"
      ],
      "metadata": {
        "id": "OleeICm6PHwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords[:15]"
      ],
      "metadata": {
        "id": "d3IpClm8O7W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stoppwörter ausschließen:"
      ],
      "metadata": {
        "id": "feGPtV0uQA9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_ges_op_osw = [w for w in tokens_ges_op if w not in stopwords]"
      ],
      "metadata": {
        "id": "epdTdTkJQDmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neuer Frequency Table:"
      ],
      "metadata": {
        "id": "my1DDj0sQ2GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_ges_op_osw = nltk.FreqDist(w.lower() for w in tokens_ges_op_osw)"
      ],
      "metadata": {
        "id": "fLmT-VR5Q5n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_ges_op_osw.most_common(20)"
      ],
      "metadata": {
        "id": "fEiOIoCyRCWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency Table für alle Autoren machen:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0gFYAHpeFDzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist = {}\n",
        "\n",
        "for cats in ms_corpus.categories():\n",
        "  text_li = []\n",
        "  for fileid in ms_corpus.fileids(categories=cats):\n",
        "    text = list(ms_corpus.words(fileids=fileid))\n",
        "    text_li = text_li + text\n",
        "\n",
        "  freq_dist[cats] = nltk.FreqDist([w.lower() for w in text_li])"
      ],
      "metadata": {
        "id": "OoQFgCNBFI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "freq_dist ist ein sogenanntes Dictionary, auf das mit dem Key (hier einfach der Autorname) zurückgegriffen werden kann:\n",
        "\n"
      ],
      "metadata": {
        "id": "tzWJbxQ0L0xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist['Walther'].most_common(20)"
      ],
      "metadata": {
        "id": "cCsEkWCdG9Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist['Walther'].freq('leit')"
      ],
      "metadata": {
        "id": "REXoDAQfMCn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kollokationen im Text finden\n",
        "\n",
        "Kollokationen sind Wörter, die gemeinsam im Text auftreten (in einem bestimmten Kontext-Window, siehe Parameter `window_size`)"
      ],
      "metadata": {
        "id": "08P_OQyJRU0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()"
      ],
      "metadata": {
        "id": "262zD1ntTDFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kollokationsliste erstellen:"
      ],
      "metadata": {
        "id": "eQSqmdExVK_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w.lower() for w in tokens_ges]\n",
        "\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(words, window_size=2)\n",
        "\n",
        "bigramFinder.apply_word_filter(lambda w: not w.isalpha())"
      ],
      "metadata": {
        "id": "n68ZpJQwRw8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... und sortieren nach häufigster Kollokation:"
      ],
      "metadata": {
        "id": "LuSwOSRtVONF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigramratio_ges = bigramFinder.score_ngrams(bigram_measures.raw_freq)"
      ],
      "metadata": {
        "id": "3s6_xi3HTabx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigramratio_ges[:20]"
      ],
      "metadata": {
        "id": "Eer6mloOTi1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Ergebnisse sind wenig aussagekräftig, da Wörter, die häufig auftreten, auch oft kookurrieren.\n",
        "\n",
        "Abhilfe: Ranking der Liste nach Likelihood-Maß, das Überzufälligkeit der Kollokation berücksichtigt."
      ],
      "metadata": {
        "id": "ywYGJ0jnURNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigramratio_ges = bigramFinder.score_ngrams(bigram_measures.likelihood_ratio)"
      ],
      "metadata": {
        "id": "ILFWzPxRSLXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigramratio_ges[:20]"
      ],
      "metadata": {
        "id": "d8qx7V_FSm3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schließlich noch möglich: Ausschluss von Stoppwörtern:"
      ],
      "metadata": {
        "id": "lDHIYUWjSKaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w.lower() for w in tokens_ges]\n",
        "\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(words, window_size=2)\n",
        "\n",
        "bigramFinder.apply_word_filter(lambda w: not w.isalpha() or w in stopwords )"
      ],
      "metadata": {
        "id": "G3ouhq3mUqRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigramratio_ges[:20]"
      ],
      "metadata": {
        "id": "E2pY2T5QU7d6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}